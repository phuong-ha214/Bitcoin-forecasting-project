# Bitcoin-forecasting-project
ğŸ“Œ Project Overview

This project builds a time-series forecasting system for Bitcoin daily log-returns using:

ARIMA (traditional statistical model)

XGBoost (tree-based ML model)

LSTM (deep learning sequence model)

Stacking meta-model (Ridge Regression)

The workflow produces out-of-sample predictions for each base model, evaluates performance, and combines them using stacking for improved accuracy.

ğŸš€ 1. Setup Instructions
Clone the repository
git clone https://github.com/phuong-ha214/Bitcoin-forecasting-project.git
cd Bitcoin-forecasting-project

ğŸ“¦ Create the environment

Install required packages:

pip install -r requirements.txt


If you are using Google Colab, simply upload or mount your project folder and run the notebooks.

ğŸ“Š 2. Dataset

The raw dataset is stored in:

data/raw/btc_1d_data_2018_to_2025.csv


It contains:

Open/High/Low/Close prices

Volume

Timestamps

You may replace this file with updated data as long as the column structure remains the same.

ğŸ§¹ 3. Preprocessing & Feature Engineering
Run the preprocessing notebook:
notebooks/1_preprocessing.ipynb


This notebook:

âœ” Loads raw BTC data
âœ” Sorts by time
âœ” Creates log-returns
âœ” Generates lag features
âœ” Normalizes features (fit on train only)
âœ” Splits data into train/test
âœ” Saves processed files into:

data/processed/btc_train.csv
data/processed/btc_test.csv

ğŸ¤– 4. Train Base Models
A. ARIMA

Run:

notebooks/2_train_arima.ipynb


This notebook:

âœ” Fits an ARIMA model on training returns
âœ” Uses auto_arima to select optimal order
âœ” Produces out-of-sample predictions aligned with test set
âœ” Saves predictions to:

results/arima_preds_train.csv
results/arima_preds_test.csv

B. XGBoost

Run:

notebooks/3_train_xgboost.ipynb


This notebook:

âœ” Loads train/test engineered features
âœ” Trains XGBoost regressor
âœ” Uses TimeSeriesSplit cross-validation
âœ” Outputs predictions to:

results/xgb_preds_train.csv
results/xgb_preds_test.csv

C. LSTM

Run:

notebooks/4_train_lstm.ipynb


This notebook:

âœ” Builds LSTM with Keras
âœ” Uses 5-fold TimeSeriesSplit to generate out-of-sample predictions for stacking
âœ” Produces final test predictions
âœ” Saves results:

results/lstm_preds_train.csv
results/lstm_preds_test.csv

ğŸ§  5. Train the Stacking Meta-Model

Run:

notebooks/5_train_stacking.ipynb


This notebook:

âœ” Loads the 3 base model predictions
âœ” Concatenates them into a feature matrix
âœ” Fits Ridge Regression meta-model
âœ” Generates final ensemble predictions
âœ” Evaluates test performance
âœ” Saves stacked outputs:

results/stacked_preds_test.csv
results/metrics_summary.csv

ğŸ“ˆ 6. Visualizations

Run:

notebooks/6_plot_results.ipynb


It will generate:

True vs Predicted returns

Rolling error curves

Model comparison table

Stacking improvement plots

Saved into:

results/plots/

ğŸ“ 7. Reproducing Results (Step-by-Step Summary)
Run these notebooks in order:

âœ” 1_preprocessing.ipynb

âœ” 2_train_arima.ipynb

âœ” 3_train_xgboost.ipynb

âœ” 4_train_lstm.ipynb

âœ” 5_train_stacking.ipynb

âœ” 6_plot_results.ipynb

After running all, you will obtain:

ARIMA / XGBoost / LSTM model performance

Final stacked model performance

Visualizations + evaluation metrics

Final prediction CSV files

ğŸ§© Notes

All models use training data only to avoid leakage.

All cross-validated predictions are out-of-sample to ensure fair stacking.

You can adjust hyperparameters in each notebook.
